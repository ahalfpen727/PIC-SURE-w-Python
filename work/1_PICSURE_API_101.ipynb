{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIC-SURE API use-case: quick analysis on COPDgene data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial notebook, aimed to be quickly up and running with the python PIC-SURE API. It covers the main functionalities of the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIC-SURE python API \n",
    "### What is PIC-SURE? \n",
    "\n",
    "<!--img src=\"./img/PIC-SURE_logo.png\" width= \"360px\"> -->\n",
    "\n",
    "As part of the BioData Catalyst initiative, the Patient Information Commons Standard Unification of Research Elements (PIC-SURE) platform has been integrating clinical and genomic datasets from multiple TOPMed and TOPMed related studies funded by the National Heart Lung and Blood Institute (NHLBI). \n",
    "\n",
    "Original data exposed through PIC-SURE API encompass a large heterogeneity of data organization underneath. PIC-SURE hide this complexity and exposes the different studies dataset in a single tabular format. By easing the process of data extraction, it allows investigators to focus on the downstream analyses and facilitate reproducible sciences.\n",
    "\n",
    "Currently, only phenotypic variables are accessible through the PIC-SURE API, but access to genomic variables is coming soon.\n",
    "\n",
    "### More about PIC-SURE\n",
    "The API is available in two different programming languages, python and R, enabling investigators to query the databases the same way using any of those languages.\n",
    "\n",
    "PIC-SURE is a larger project from which the R/python PIC-SURE API is only a brick. Among other things, PIC-SURE also offers a graphical user interface that allows researchers to explore variables across multiple studies, filter patient that match criteria, and create cohort from this interactive exploration.\n",
    "\n",
    "The python API is actively developed by the Avillach-Lab at Harvard Medical School.\n",
    "\n",
    "PIC-SURE API GitHub repo:\n",
    "* https://github.com/hms-dbmi/pic-sure-python-adapter-hpds\n",
    "* https://github.com/hms-dbmi/pic-sure-python-client\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -------   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting your own user-specific security token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running this notebook, please be sure to review the `get_your_token.ipynb` notebook. It contains explanation about how to get a security token, mandatory to access the databases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite\n",
    "- python 3.6 or later\n",
    "- pip python package manager, already available in most systems with a python interpreter installed ([pip installation instructions](https://pip.pypa.io/en/stable/installing/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages installation\n",
    "\n",
    "Installation of the packages listed in the `requirements.txt` file, as well as the two components of the PIC-SURE API from GitHub, that is the PIC-SURE adapter and the PIC-SURE Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib>=3.1.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: pandas>=0.25.3 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 3)) (0.25.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: statsmodels>=0.10.2 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 6)) (0.11.1)\n",
      "Collecting numpy==1.16.4\n",
      "  Using cached numpy-1.16.4.zip (5.1 MB)\n",
      "Collecting tqdm>=4.38.0\n",
      "  Using cached tqdm-4.54.1-py2.py3-none-any.whl (69 kB)\n",
      "Building wheels for collected packages: numpy\n",
      "  Building wheel for numpy (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-m9xzwjpx\n",
      "       cwd: /tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/\n",
      "  Complete output (332 lines):\n",
      "  Running from numpy source directory.\n",
      "  /tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/misc_util.py:476: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "    return is_string(s) and ('*' in s or '?' is s)\n",
      "  blas_opt_info:\n",
      "  blas_mkl_info:\n",
      "  customize UnixCCompiler\n",
      "    libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  blis_info:\n",
      "  customize UnixCCompiler\n",
      "    libraries blis not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  openblas_info:\n",
      "  customize UnixCCompiler\n",
      "  customize UnixCCompiler\n",
      "    libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  atlas_3_10_blas_threads_info:\n",
      "  Setting PTATLAS=ATLAS\n",
      "  customize UnixCCompiler\n",
      "    libraries tatlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  atlas_3_10_blas_info:\n",
      "  customize UnixCCompiler\n",
      "    libraries satlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  atlas_blas_threads_info:\n",
      "  Setting PTATLAS=ATLAS\n",
      "  customize UnixCCompiler\n",
      "    libraries ptf77blas,ptcblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  atlas_blas_info:\n",
      "  customize UnixCCompiler\n",
      "    libraries f77blas,cblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  accelerate_info:\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  /tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/system_info.py:639: UserWarning:\n",
      "      Atlas (http://math-atlas.sourceforge.net/) libraries not found.\n",
      "      Directories to search for the libraries can be specified in the\n",
      "      numpy/distutils/site.cfg file (section [atlas]) or by setting\n",
      "      the ATLAS environment variable.\n",
      "    self.calc_info()\n",
      "  blas_info:\n",
      "  customize UnixCCompiler\n",
      "    libraries blas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  /tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/system_info.py:639: UserWarning:\n",
      "      Blas (http://www.netlib.org/blas/) libraries not found.\n",
      "      Directories to search for the libraries can be specified in the\n",
      "      numpy/distutils/site.cfg file (section [blas]) or by setting\n",
      "      the BLAS environment variable.\n",
      "    self.calc_info()\n",
      "  blas_src_info:\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  /tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/system_info.py:639: UserWarning:\n",
      "      Blas (http://www.netlib.org/blas/) sources not found.\n",
      "      Directories to search for the sources can be specified in the\n",
      "      numpy/distutils/site.cfg file (section [blas_src]) or by setting\n",
      "      the BLAS_SRC environment variable.\n",
      "    self.calc_info()\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  /bin/sh: 1: svnversion: not found\n",
      "  non-existing path in 'numpy/distutils': 'site.cfg'\n",
      "  lapack_opt_info:\n",
      "  lapack_mkl_info:\n",
      "  customize UnixCCompiler\n",
      "    libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  openblas_lapack_info:\n",
      "  customize UnixCCompiler\n",
      "  customize UnixCCompiler\n",
      "    libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  openblas_clapack_info:\n",
      "  customize UnixCCompiler\n",
      "  customize UnixCCompiler\n",
      "    libraries openblas,lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  atlas_3_10_threads_info:\n",
      "  Setting PTATLAS=ATLAS\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/local/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries tatlas,tatlas not found in /usr/local/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries tatlas,tatlas not found in /usr/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib/x86_64-linux-gnu/atlas\n",
      "  customize UnixCCompiler\n",
      "    libraries tatlas,tatlas not found in /usr/lib/x86_64-linux-gnu/atlas\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib/x86_64-linux-gnu\n",
      "  customize UnixCCompiler\n",
      "    libraries tatlas,tatlas not found in /usr/lib/x86_64-linux-gnu\n",
      "  <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  atlas_3_10_info:\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/local/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries satlas,satlas not found in /usr/local/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries satlas,satlas not found in /usr/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib/x86_64-linux-gnu/atlas\n",
      "  customize UnixCCompiler\n",
      "    libraries satlas,satlas not found in /usr/lib/x86_64-linux-gnu/atlas\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib/x86_64-linux-gnu\n",
      "  customize UnixCCompiler\n",
      "    libraries satlas,satlas not found in /usr/lib/x86_64-linux-gnu\n",
      "  <class 'numpy.distutils.system_info.atlas_3_10_info'>\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  atlas_threads_info:\n",
      "  Setting PTATLAS=ATLAS\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/local/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries ptf77blas,ptcblas,atlas not found in /usr/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib/x86_64-linux-gnu/atlas\n",
      "  customize UnixCCompiler\n",
      "    libraries ptf77blas,ptcblas,atlas not found in /usr/lib/x86_64-linux-gnu/atlas\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib/x86_64-linux-gnu\n",
      "  customize UnixCCompiler\n",
      "    libraries ptf77blas,ptcblas,atlas not found in /usr/lib/x86_64-linux-gnu\n",
      "  <class 'numpy.distutils.system_info.atlas_threads_info'>\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  atlas_info:\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/local/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries f77blas,cblas,atlas not found in /usr/local/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries f77blas,cblas,atlas not found in /usr/lib\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib/x86_64-linux-gnu/atlas\n",
      "  customize UnixCCompiler\n",
      "    libraries f77blas,cblas,atlas not found in /usr/lib/x86_64-linux-gnu/atlas\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack_atlas not found in /usr/lib/x86_64-linux-gnu\n",
      "  customize UnixCCompiler\n",
      "    libraries f77blas,cblas,atlas not found in /usr/lib/x86_64-linux-gnu\n",
      "  <class 'numpy.distutils.system_info.atlas_info'>\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  lapack_info:\n",
      "  customize UnixCCompiler\n",
      "    libraries lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  /tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/system_info.py:639: UserWarning:\n",
      "      Lapack (http://www.netlib.org/lapack/) libraries not found.\n",
      "      Directories to search for the libraries can be specified in the\n",
      "      numpy/distutils/site.cfg file (section [lapack]) or by setting\n",
      "      the LAPACK environment variable.\n",
      "    self.calc_info()\n",
      "  lapack_src_info:\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  /tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/system_info.py:639: UserWarning:\n",
      "      Lapack (http://www.netlib.org/lapack/) sources not found.\n",
      "      Directories to search for the sources can be specified in the\n",
      "      numpy/distutils/site.cfg file (section [lapack_src]) or by setting\n",
      "      the LAPACK_SRC environment variable.\n",
      "    self.calc_info()\n",
      "    NOT AVAILABLE\n",
      "  \n",
      "  /usr/lib/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'define_macros'\n",
      "    warnings.warn(msg)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running config_cc\n",
      "  unifing config_cc, config, build_clib, build_ext, build commands --compiler options\n",
      "  running config_fc\n",
      "  unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\n",
      "  running build_src\n",
      "  build_src\n",
      "  building py_modules sources\n",
      "  creating build\n",
      "  creating build/src.linux-x86_64-3.8\n",
      "  creating build/src.linux-x86_64-3.8/numpy\n",
      "  creating build/src.linux-x86_64-3.8/numpy/distutils\n",
      "  building library \"npymath\" sources\n",
      "  get_default_fcompiler: matching types: '['gnu95', 'intel', 'lahey', 'pg', 'absoft', 'nag', 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 'pathf95', 'nagfor']'\n",
      "  customize Gnu95FCompiler\n",
      "  Found executable /usr/bin/gfortran\n",
      "  customize Gnu95FCompiler\n",
      "  customize Gnu95FCompiler using config\n",
      "  C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \n",
      "  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.8 -c'\n",
      "  x86_64-linux-gnu-gcc: _configtest.c\n",
      "  x86_64-linux-gnu-gcc -pthread _configtest.o -o _configtest\n",
      "  success!\n",
      "  removing: _configtest.c _configtest.o _configtest.o.d _configtest\n",
      "  C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \n",
      "  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.8 -c'\n",
      "  x86_64-linux-gnu-gcc: _configtest.c\n",
      "  _configtest.c:1:5: warning: conflicting types for built-in function ‘exp’; expected ‘double(double)’ [-Wbuiltin-declaration-mismatch]\n",
      "      1 | int exp (void);\n",
      "        |     ^~~\n",
      "  _configtest.c:1:1: note: ‘exp’ is declared in header ‘<math.h>’\n",
      "    +++ |+#include <math.h>\n",
      "      1 | int exp (void);\n",
      "  x86_64-linux-gnu-gcc -pthread _configtest.o -o _configtest\n",
      "  /usr/bin/ld: _configtest.o: in function `main':\n",
      "  /tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/_configtest.c:6: undefined reference to `exp'\n",
      "  collect2: error: ld returned 1 exit status\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o _configtest.o.d\n",
      "  C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \n",
      "  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.8 -c'\n",
      "  x86_64-linux-gnu-gcc: _configtest.c\n",
      "  _configtest.c:1:5: warning: conflicting types for built-in function ‘exp’; expected ‘double(double)’ [-Wbuiltin-declaration-mismatch]\n",
      "      1 | int exp (void);\n",
      "        |     ^~~\n",
      "  _configtest.c:1:1: note: ‘exp’ is declared in header ‘<math.h>’\n",
      "    +++ |+#include <math.h>\n",
      "      1 | int exp (void);\n",
      "  x86_64-linux-gnu-gcc -pthread _configtest.o -lm -o _configtest\n",
      "  success!\n",
      "  removing: _configtest.c _configtest.o _configtest.o.d _configtest\n",
      "  creating build/src.linux-x86_64-3.8/numpy/core\n",
      "  creating build/src.linux-x86_64-3.8/numpy/core/src\n",
      "  creating build/src.linux-x86_64-3.8/numpy/core/src/npymath\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/npymath/npy_math_internal.h\n",
      "    adding 'build/src.linux-x86_64-3.8/numpy/core/src/npymath' to include_dirs.\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/npymath/ieee754.c\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/npymath/npy_math_complex.c\n",
      "  None - nothing done with h_files = ['build/src.linux-x86_64-3.8/numpy/core/src/npymath/npy_math_internal.h']\n",
      "  building library \"npysort\" sources\n",
      "  creating build/src.linux-x86_64-3.8/numpy/core/src/common\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/common/npy_sort.h\n",
      "    adding 'build/src.linux-x86_64-3.8/numpy/core/src/common' to include_dirs.\n",
      "  creating build/src.linux-x86_64-3.8/numpy/core/src/npysort\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/npysort/quicksort.c\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/npysort/mergesort.c\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/npysort/heapsort.c\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/common/npy_partition.h\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/npysort/selection.c\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/common/npy_binsearch.h\n",
      "  conv_template:> build/src.linux-x86_64-3.8/numpy/core/src/npysort/binsearch.c\n",
      "  None - nothing done with h_files = ['build/src.linux-x86_64-3.8/numpy/core/src/common/npy_sort.h', 'build/src.linux-x86_64-3.8/numpy/core/src/common/npy_partition.h', 'build/src.linux-x86_64-3.8/numpy/core/src/common/npy_binsearch.h']\n",
      "  building extension \"numpy.core._dummy\" sources\n",
      "  Generating build/src.linux-x86_64-3.8/numpy/core/include/numpy/config.h\n",
      "  C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \n",
      "  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.8 -c'\n",
      "  x86_64-linux-gnu-gcc: _configtest.c\n",
      "  _configtest.c:1:10: fatal error: Python.h: No such file or directory\n",
      "      1 | #include <Python.h>\n",
      "        |          ^~~~~~~~~~\n",
      "  compilation terminated.\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/setup.py\", line 415, in <module>\n",
      "      setup_package()\n",
      "    File \"/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/setup.py\", line 407, in setup_package\n",
      "      setup(**metadata)\n",
      "    File \"/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/core.py\", line 171, in setup\n",
      "      return old_setup(**new_attr)\n",
      "    File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 144, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"/usr/lib/python3.8/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/usr/lib/python3.8/distutils/dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/usr/lib/python3/dist-packages/wheel/bdist_wheel.py\", line 223, in run\n",
      "      self.run_command('build')\n",
      "    File \"/usr/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/command/build.py\", line 47, in run\n",
      "      old_build.run(self)\n",
      "    File \"/usr/lib/python3.8/distutils/command/build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"/usr/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/command/build_src.py\", line 148, in run\n",
      "      self.build_sources()\n",
      "    File \"/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/command/build_src.py\", line 165, in build_sources\n",
      "      self.build_extension_sources(ext)\n",
      "    File \"/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/command/build_src.py\", line 322, in build_extension_sources\n",
      "      sources = self.generate_sources(sources, ext)\n",
      "    File \"/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/numpy/distutils/command/build_src.py\", line 375, in generate_sources\n",
      "      source = func(extension, build_dir)\n",
      "    File \"numpy/core/setup.py\", line 423, in generate_config_h\n",
      "      moredefs, ignored = cocache.check_types(config_cmd, ext, build_dir)\n",
      "    File \"numpy/core/setup.py\", line 47, in check_types\n",
      "      out = check_types(*a, **kw)\n",
      "    File \"numpy/core/setup.py\", line 279, in check_types\n",
      "      raise SystemError(\n",
      "  SystemError: Cannot compile 'Python.h'. Perhaps you need to install python-dev|python-devel.\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for numpy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' clean --all\n",
      "       cwd: /tmp/pip-install-u0bydm7v/numpy_9ff7f43bb233458f9c5e07d89f79b852\n",
      "  Complete output (10 lines):\n",
      "  Running from numpy source directory.\n",
      "  \n",
      "  `setup.py clean` is not supported, use one of the following instead:\n",
      "  \n",
      "    - `git clean -xdf` (cleans all files)\n",
      "    - `git clean -Xdf` (cleans all versioned files, doesn't touch\n",
      "                        files that aren't checked into the git repo)\n",
      "  \n",
      "  Add `--force` to your command to use it anyway if you must (unsupported).\n",
      "  \n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed cleaning build dir for numpy\u001b[0m\n",
      "Failed to build numpy\n",
      "Installing collected packages: tqdm, numpy\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.30.0\n",
      "    Uninstalling tqdm-4.30.0:\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/usr/bin/tqdm'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/hms-dbmi/pic-sure-python-adapter-hpds.git\n",
      "  Cloning https://github.com/hms-dbmi/pic-sure-python-adapter-hpds.git to /tmp/pip-req-build-pz266pbf\n",
      "Collecting httplib2\n",
      "  Using cached httplib2-0.18.1-py3-none-any.whl (95 kB)\n",
      "Building wheels for collected packages: PicSureHpdsLib\n",
      "  Building wheel for PicSureHpdsLib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PicSureHpdsLib: filename=PicSureHpdsLib-0.9.0-py2.py3-none-any.whl size=21879 sha256=83afb0891538673037c26f0aa052646a0a464e94280eef92b826180f38642037\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zfsj48c3/wheels/e8/35/43/484d5d574661fc4a2c5b083551bc3c7254695764ed17ce397e\n",
      "Successfully built PicSureHpdsLib\n",
      "Installing collected packages: httplib2, PicSureHpdsLib\n",
      "  Attempting uninstall: httplib2\n",
      "    Found existing installation: httplib2 0.18.1\n",
      "    Uninstalling httplib2-0.18.1:\n",
      "      Successfully uninstalled httplib2-0.18.1\n",
      "  Attempting uninstall: PicSureHpdsLib\n",
      "    Found existing installation: PicSureHpdsLib 0.9.0\n",
      "    Uninstalling PicSureHpdsLib-0.9.0:\n",
      "      Successfully uninstalled PicSureHpdsLib-0.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "launchpadlib 1.10.13 requires testresources, which is not installed.\u001b[0m\n",
      "Successfully installed PicSureHpdsLib-0.9.0 httplib2-0.18.1\n",
      "Collecting git+https://github.com/hms-dbmi/pic-sure-python-client.git\n",
      "  Cloning https://github.com/hms-dbmi/pic-sure-python-client.git to /tmp/pip-req-build-itu3y6vx\n",
      "Building wheels for collected packages: PicSureClient\n",
      "  Building wheel for PicSureClient (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PicSureClient: filename=PicSureClient-0.1.0-py2.py3-none-any.whl size=10146 sha256=44c63e7ecbeefae4405f4c17f49db4ca384eab018110eb1b3c37f934c94a1a75\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xey1lso4/wheels/2f/61/65/c5af5ad47fa6a9e191522e1ae06d74c08b05c91a76a1c3d120\n",
      "Successfully built PicSureClient\n",
      "Installing collected packages: PicSureClient\n",
      "  Attempting uninstall: PicSureClient\n",
      "    Found existing installation: PicSureClient 0.1.0\n",
      "    Uninstalling PicSureClient-0.1.0:\n",
      "      Successfully uninstalled PicSureClient-0.1.0\n",
      "Successfully installed PicSureClient-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-python-adapter-hpds.git\n",
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-python-client.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the external dependencies, as well as user-defined functions stored in the `python_lib` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import PicSureHpdsLib\n",
    "import PicSureClient\n",
    "\n",
    "from utils import get_multiIndex_variablesDict, joining_variablesDict_onCol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting the display parameter for tables and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame display options\n",
    "pd.set_option(\"max.rows\", 100)\n",
    "\n",
    "# Matplotlib display parameters\n",
    "plt.rcParams[\"figure.figsize\"] = (14,8)\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to a PIC-SURE resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several information are required to get access to data through the PIC-SURE API: a network URL, a resource id, and a user-specific security token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICSURE_network_URL = \"https://picsure.biodatacatalyst.nhlbi.nih.gov/picsure\"\n",
    "resource_id = \"02e23f52-f354-4e8b-992c-d37c8b9ba140\"\n",
    "token_file = \"token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(token_file, \"r\") as f:\n",
    "    my_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;91;40m\n",
      "\n",
      "+=========================================================================================+\n",
      "|        [ WARNING ] you are specifying that you WANT to allow self-signed SSL            |\n",
      "|        certificates to be acceptable for connections.  This may be useful for           |\n",
      "|        working in a development environment or on systems that host public              |\n",
      "|        data.  BEST SECURITY PRACTICES ARE THAT IF YOU ARE WORKING WITH SENSITIVE        |\n",
      "|        DATA THEN ALL SSL CERTS BY THOSE EVIRONMENTS SHOULD NOT BE SELF-SIGNED.          |\n",
      "+=========================================================================================+\n",
      "\u001b[39;49m\n",
      "+--------------------------------------+------------------------------------------------------\n",
      "|  Resource UUID                       |  Resource Name                                  \n",
      "+--------------------------------------+------------------------------------------------------\n",
      "| ERROR:                             \n",
      "|     User is not authorized. [Token invalid or expired]\n",
      "+--------------------------------------+------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Resource UUID \"02e23f52-f354-4e8b-992c-d37c8b9ba140\" was not found!'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-082a1a5e06f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPICSURE_network_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0madapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPicSureHpdsLib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0museResource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PicSureHpdsLib/PicSureHpds.py\u001b[0m in \u001b[0;36museResource\u001b[0;34m(self, resource_uuid)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mHpdsResourceConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_reference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Resource UUID \"'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\" was not found!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munlockResource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_uuid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Resource UUID \"02e23f52-f354-4e8b-992c-d37c8b9ba140\" was not found!'"
     ]
    }
   ],
   "source": [
    "client = PicSureClient.Client()\n",
    "connection = client.connect(PICSURE_network_URL, my_token, True)\n",
    "adapter = PicSureHpdsLib.Adapter(connection)\n",
    "resource = adapter.useResource(resource_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two objects are created here: a `connection` and a `resource` object.\n",
    "\n",
    "As we will only be using one single resource, **the `resource` object is actually the only one we will need to proceed with data analysis hereafter**. \n",
    "\n",
    "It is connected to the specific data source ID we specified, and enables to query and retrieve data from this database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting help with the PIC-SURE API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each object exposed by the PicSureHpdsLib library got a `help()` method. Calling it will without parameters print out an information about functionalities of this object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource.help()\n",
    "disease_var = variablesDict.index.get_level_values(0) == \"disease\"\n",
    "age_var = variablesDict.index.get_level_values(1) == \"AGE\"\n",
    "lung_cancer = variablesDict.loc[mask_lung_cancer, \"varName\"] \n",
    "ever_asthma = variablesDict.loc[mask_ever_asthma, \"varName\"] \n",
    "current_asthma = variablesDict.loc[mask_current_asthma, \"varName\"] \n",
    "age_var = variablesDict.loc[ages, \"varName\"]\n",
    "disease_var = variablesDict.loc[disease, \"varName\"]\n",
    "\n",
    "# Getting variable names to filter query on\n",
    "mask_breath = variablesDict[\"simplified_varName\"] == \"Ever asthma?\"\n",
    "mask_breath = variablesDict[\"simplified_varName\"] == \"Current asthma?\"\n",
    "mask_breath = variablesDict[\"simplified_varName\"] == \"lung_cancer_self_report\"\n",
    "values_stroke_post_HCT = variablesDict.loc[mask_stroke, \"categoryValues\"]\n",
    "\n",
    "lung_cancer = plain_variablesDict.loc[mask_lung_cancer, \"varName\"] \n",
    "ever_asthma = plain_variablesDict.loc[mask_ever_asthma, \"varName\"] \n",
    "current_asthma = plain_variablesDict.loc[mask_current_asthma, \"varName\"] \n",
    "age_var = variablesDict.loc[ages, \"varName\"]\n",
    "disease_var = variablesDict.loc[disease, \"varName\"]\n",
    "\n",
    "my_query.filter().add(disease, values=mask_breath)\n",
    "my_query.select().add(varnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this output tells us that this `resource` object has 3 methods, and it gives a quick definition of those methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the *variables dictionnary*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a connection to the desired resource has been established, we first need to get a knowledge of which variables are available in the database. To this end, we will use the `dictionary` method of the `resource` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `dictionary` instance enables to retrieve matching records by searching for a specific term, or to retrieve information about all the available variables, using the `find()` method. For instance, looking for variables containing the term `COPD` in their names is done this way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e2585a709333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdictionary_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"breath\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resource' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary = resource.dictionary()\n",
    "dictionary_search = dictionary.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, objects created by the `dictionary.find` method expose the search results via 4 different methods: `.count()`, `.keys()`, `.entries()`, and `.DataFrame()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint({\"Count\": dictionary_search.count(), \n",
    "        \"Keys\": dictionary_search.keys()[0:5],\n",
    "        \"Entries\": dictionary_search.entries()[0:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_search.DataFrame().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `.DataFrame()` method enables to get the result of the dictionary search in a pandas DataFrame format. This way, it allows to:** \n",
    "\n",
    "\n",
    "* Use the various information exposed in the dictionary (patient count, variable type ...) as criteria for variable selection.\n",
    "* Use the row names of the DataFrame to get the actual variables names, to be used in the query, as shown below.\n",
    "\n",
    "Variable names, aren't very pratical to use right away, for two reasons:\n",
    "1. Very long\n",
    "2. Presence of backslashes that prevent from copy-pasting. \n",
    "\n",
    "However, retrieving the dictionary search result in the form of a dataframe can help to deal with this, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_variablesDict = resource.dictionary().find().DataFrame()\n",
    "plain_variablesDict.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, using the `dictionary.find()` function without arguments return every entries, as shown in the help documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource.dictionary().help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_variablesDict.iloc[10:20,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary currently returned by the API provides information about the variables, such as:\n",
    "- observationCount: number of entries with non-null value\n",
    "- categorical: type of the variables, True if strings, False if numerical\n",
    "- min/max: only provided for numerical variables\n",
    "- HpdsDataType: 'phenotypes' or 'genotypes'. Currently, the API only expsoses'phenotypes' variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Full Data Dictionary to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to export the data dictionary first we will create a Pandas dataframe called fullVariableDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullVariableDict = resource.dictionary().find().DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the fullVariableDict dataframe contains some values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullVariableDict.iloc[0:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullVariableDict.to_csv('data_dictionary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a data_dictionary.csv in the file explorer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable dictionary + pandas multiIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a simple user-defined function (`get_multiIndex_variablesDict`) to add a little more information to the variable dictionary and to simplify working with variables names. It takes advantage of pandas MultiIndex functionality [see pandas official documentation on this topic](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html).\n",
    "\n",
    "Although not an official feature of the API, such functionality illustrates how to quickly select groups of related variables.\n",
    "\n",
    "Printing the 'multiIndexed' variable Dictionary allows to quickly see the tree-like organisation of the variable names. Moreover, original and simplified variable names are now stored respectively in the \"varName\" and \"simplified_varName\" columns (simplified variable names is simply the last component of the variable name, that is usually the most informative to know what each variable is about)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_multiIndex_variablesDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5dec34c125ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvariablesDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_multiIndex_variablesDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplain_variablesDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_multiIndex_variablesDict' is not defined"
     ]
    }
   ],
   "source": [
    "variablesDict = get_multiIndex_variablesDict(plain_variablesDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variablesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have seen how our entire dictionnary looked, we limit the number of lines to be displayed for the future outputs\n",
    "pd.set_option(\"max.rows\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple example to illustrate the simplicity of use a multiIndex dictionary. Let's say we are interested in every variables pertaining to the \"Medical history\" and \"Medication history\" subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_medication = variablesDict.index.get_level_values(2) == \"Medication History\"\n",
    "mask_medical = variablesDict.index.get_level_values(2) == \"Medical History\"\n",
    "medication_history_variables = variablesDict.loc[mask_medical | mask_medication,:]\n",
    "medication_history_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although pretty simple, it can be easily combined with other filters to quickly select desired group of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying and retrieving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside from the dictionary, the second cornerstone of the API is the `query` object. It is the entering point to retrieve data from the resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a query object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = resource.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query object has several methods that enable to build a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `query.select.add()` method accepts variable names as string or list of strings as argument, and will allow the query to return all variables included in the list, without any record (ie subjects/rows) subsetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `query.require.add()` method accepts variable names as string or list of strings as argument, and will allow the query to return all the variables passed, and only records that do not contain any null values for those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `query.anyof.add()` method accepts variable names as string or list of strings as argument, and will allow the query to return all variables included in the list, and only records that do contain at least one non-null value for those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `query.filter.add()` method accepts a variable name as argument, plus additional values to filter on that given variable. The query will return this variable and only the records that do match this filter criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All those 4 methods can be combined when building a query. The record eventually returned by the query have to meet all the different specified filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = variablesDict[\"simplified_name\"] == \"How old were you when you completely stopped smoking? [Years old]\"\n",
    "yo_stop_smoking_varname = variablesDict.loc[mask, \"name\"] \n",
    "\n",
    "mask_cat = variablesDict[\"categorical\"] == True\n",
    "mask_count = variablesDict[\"observationCount\"].between(100,2000)\n",
    "varnames = variablesDict.loc[mask_cat & mask_count, \"name\"]\n",
    "\n",
    "my_query.filter().add(yo_stop_smoking_varname, min=20, max=70)\n",
    "my_query.select().add(varnames[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Selecting Consent Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime it can be necessary to limit results to a group of patients that have provided common consent types. By default, PIC-SURE will enforce limits to the consents that each researcher has individually been authorized for, however it may be desirable to further restrict the results. To view the available consent groups, you can use the query.filter().show() function on a new query. Look for the list of values under \"\\_Consents\\Short Study Accession with Consent Code\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource.query().filter().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to update the values, the existing list needs to be cleared first, then replaced. (phs000179.c2 is one consent code used in the COPDGene study.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query.filter().delete(\"\\\\_Consents\\\\Short Study Accession with Consent Code\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query.filter().add(\"\\\\_Consents\\\\Short Study Accession with Consent Code\\\\\", ['phs000179.c2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our query object is finally built, we use the `getResultsDataFrame` function to retrieve the data corresponding to our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = my_query.getResultsDataFrame(low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result[yo_stop_smoking_varname].plot.hist(legend=None, title= \"Age stopping smoking\", bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving data from query run through PIC-SURE UI\n",
    "\n",
    "It is possible for you to retrieve the results of a query that you have previously run using the PIC-SURE UI. To do this you must \"select data for export\", then select the information that you want the query to return and then click \"prepare data export\". Once the query is finished executing, a group of buttons will be presented.  Click the \"copy query ID to clipboard\" button to copy your unique query identifier so you can paste it into your notebook.\n",
    "\n",
    "\n",
    "Paste your query's ID into your notebook and assign it to a variable.  You then use the `query.getResults(resource, yourQueryUUID)` function with an initialized resource object to retrieve the data from your query as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The screenshot below shows the button of interest in the PIC-SURE UI. It shows that the previously run query has a DataSetID of `dce08fab-98d3-434a-937a-cb583679efe8`. At this point a copy-paste process is used to provide the DataSetID to the API, as shown in the example code below.  To run this code you must replace the example query ID with a query ID from a query that you have run in the PIC-SURE API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1kxFLxjEdMfkF4HjdWBaNju0PyMrYxGR0\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this using your notebook you must replace it with the ID value of a query that you have run.\n",
    "DataSetID = '<<replace with your QuerySetID>>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "results = resource.retrieveQueryResults(DataSetID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "df_UI = pd.read_csv(StringIO(results), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UI.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
